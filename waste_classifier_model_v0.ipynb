{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0063c6ae-bd38-4ba8-852e-3b069aadcc51",
   "metadata": {},
   "source": [
    "# Waste Classifier PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702c478-1523-4c5a-b11b-763cf43c4a31",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Create a PyTorch model that can distinguish between 6 classes:\n",
    "- cardboard\n",
    "- glass\n",
    "- metal\n",
    "- paper\n",
    "- plastic\n",
    "- trash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6471932-9f02-47aa-9011-0888c78663b4",
   "metadata": {},
   "source": [
    "## 0 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9d073d-54de-4be3-aea0-9ad4b08c61ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cpu\n",
      "0.17.1+cpu\n",
      "Base imports done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__) # 1.12+\n",
    "print(torchvision.__version__) # 0.13+\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "try:\n",
    "    from scripts import data_setup, engine, utils\n",
    "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "    print(\"Base imports done.\")\n",
    "except:\n",
    "    print(\"Couldn't find helper scripts, downloading from Github...\")\n",
    "    !git clone https://github.com/tznpau/waste-classifier\n",
    "    !mv waste-classifier/scripts .\n",
    "    !mv waste-classifier/helper_functions.py .\n",
    "    !rm -rf waste-classifier\n",
    "    from scripts import data_setup, engine, utils\n",
    "    from helper_functions import download_data, set_seeds, loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01da26d4-ab7f-4898-b6cd-fd5ca15d4181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a593538e-16a3-4317-b6ff-0900b2688dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a330bd7-64a9-45a3-8c99-fb73db35e2d9",
   "metadata": {},
   "source": [
    "## 1. Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a622d8f2-0291-48b0-a272-26093b8e277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data\\trash_dataset directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/trash_dataset')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trash_dataset_path = download_data(source=\"https://github.com/tznpau/waste-classifier/raw/main/data/trash_dataset.zip\",\n",
    "                                   destination=\"trash_dataset\")\n",
    "trash_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee2ccfd-fb89-418e-946c-821aea43760e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/trash_dataset/train'),\n",
       " WindowsPath('data/trash_dataset/test'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup training and test data directories\n",
    "train_dir = trash_dataset_path / \"train\"\n",
    "test_dir = trash_dataset_path / \"test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66780a37-782d-4caf-9a08-93e0996d5829",
   "metadata": {},
   "source": [
    "## 2. Deployment questions\n",
    "1. ideal model scenario ?\n",
    "2. where is the model going to go ?\n",
    "3. how is the model going to function ?\n",
    "\n",
    "**ideal use case**\n",
    "- performs at 90%+ accuracy\n",
    "- performs fast: 30fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35245ef2-38af-4fa7-8876-e9cec0fcd395",
   "metadata": {},
   "source": [
    "## 3. EffNetB2 feature extractor\n",
    "\n",
    "https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ea56e3-ae25-4000-a69d-dc66f06a3379",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights).to(device)\n",
    "\n",
    "# freeze base layers\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b905e5f-2986-400c-abac-e7b2e4ae4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# summary(effnetb2, \n",
    "#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "#         verbose=0,\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c52b99f-f9fd-4bdf-9fec-e87e86ef5b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=True)\n",
       "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a83bc729-f2fb-493e-926c-1346ba9ba47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "\n",
    "effnetb2.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(in_features=1408, out_features=6, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9d5100-4abc-4d7d-b3b1-be685f2b7662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.3, inplace=True)\n",
       "  (1): Linear(in_features=1408, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd3a67-6b70-4a37-bd06-c3e8003f9a27",
   "metadata": {},
   "source": [
    "So after freezing the base layer I adapted the EffNetB2 architecture to suit my use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d252f6-b1a8-4d9e-bab4-8438c26ca2c9",
   "metadata": {},
   "source": [
    "### 3.1 Creating an EffNetB2 feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f3e4eeb-191b-43cc-8ecf-a42f300a823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2_model(num_classes:int=6,\n",
    "                          seed:int=42):\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes)\n",
    "    ).to(device)\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec200a9a-7207-485f-b93f-61b8131e0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=6, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d1bbf7-b200-4dd1-b934-2169d28e1d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[288]\n",
       "    resize_size=[288]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10015aa-1c6d-4fb9-8d73-90d7af141f9b",
   "metadata": {},
   "source": [
    "### 3.2 DataLoaders for EffNetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb20bf6e-68c3-4ff8-8ef5-00ac607827f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import data_setup\n",
    "\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                  test_dir=test_dir,\n",
    "                                                                                                  transform=effnetb2_transforms,\n",
    "                                                                                                  batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb7e23-26af-4fad-8639-970daa7874de",
   "metadata": {},
   "source": [
    "Let's check that the split was done correctly during `data_setup.create_dataloaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90c10187-18e7-4cd4-a8d5-a12156c8b084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader_effnetb2), len(test_dataloader_effnetb2), class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b65caf-fa1c-44d1-b909-0e7d937d01eb",
   "metadata": {},
   "source": [
    "We have 64 batches for the train dataloader and 16 batches for the test dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b07960-2a0c-4cc3-b6e6-c8022c7508da",
   "metadata": {},
   "source": [
    "### 3.3 Training EffNetB2 feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8342b69e-18bd-454c-a152-1d96a3127fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import engine\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "# effnetb2_results = engine.train(model=effnetb2,\n",
    "#                                 train_dataloader=train_dataloader_effnetb2,\n",
    "#                                 test_dataloader=test_dataloader_effnetb2,\n",
    "#                                 epochs=EPOCHS,\n",
    "#                                 optimizer=optimizer,\n",
    "#                                 loss_fn=loss_fn,\n",
    "#                                 device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eee9ac16-b0ca-423e-9e49-42a2231c832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "# plot_loss_curves(effnetb2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd066c8-989e-4d93-8b75-d0bef092f40d",
   "metadata": {},
   "source": [
    "## 4. ViT feature extractor\n",
    "\n",
    "https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f552d55-559c-4e4f-bfda-f3ee42de68eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = torchvision.models.vit_b_16()\n",
    "vit.heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43151990-f3c2-4d4d-919f-872813b24227",
   "metadata": {},
   "source": [
    "### 4.1 Creating a ViT feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4278c7b1-44ee-420d-ae6e-e6bfc0a1e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_model(num_classes:int=6,\n",
    "                     seed:int=42):\n",
    "    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    tranasforms = weights.transforms()\n",
    "    model = torchvision.models.vit_b_16(weights=weights).to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model.heads = nn.Sequential(\n",
    "        nn.Linear(in_features=768, out_features=num_classes)\n",
    "    ).to(device)\n",
    "\n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f830805a-c986-481d-9319-93f6e4847330",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit, vit_transforms = create_vit_model(num_classes=6,\n",
    "                                       seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78370156-d799-453a-8678-7276ef67a188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 6]               768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n",
       "├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n",
       "│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n",
       "├─Sequential (heads)                                         [1, 768]             [1, 6]               --                   True\n",
       "│    └─Linear (0)                                            [1, 768]             [1, 6]               4,614                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,803,270\n",
       "Trainable params: 4,614\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (Units.MEGABYTES): 172.47\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.21\n",
       "Estimated Total Size (MB): 333.90\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print ViT feature extractor model summary (uncomment for full output)\n",
    "summary(vit, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a692a-5daa-43b4-86f1-f7f631536a60",
   "metadata": {},
   "source": [
    "### 4.2 Creating DataLoaders for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83b256fe-3458-41ca-9f26-5aedaf769211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import data_setup\n",
    "\n",
    "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                                                  test_dir=test_dir,\n",
    "                                                                                                  transform=vit_transforms,\n",
    "                                                                                                  batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9fdf029-67a9-4abf-999c-18bc4fe9142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader_vit), len(test_dataloader_vit), class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0280a3-fa1f-4f71-84ea-81f886fa8d59",
   "metadata": {},
   "source": [
    "### 4.3 Training ViT feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19d46a8d-f5e8-48e7-939f-aa3b518123c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9aa2ddbcc0401983aa1746bdd311bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'module' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mvit\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m      5\u001b[0m                              lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      7\u001b[0m set_seeds()\n\u001b[1;32m----> 9\u001b[0m vit_results \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mvit, \n\u001b[0;32m     11\u001b[0m     train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader_vit,\n\u001b[0;32m     12\u001b[0m     test_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader_vit,\n\u001b[0;32m     13\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     14\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     15\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m     16\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m~\\source\\repos\\waste-classifier\\scripts\\engine.py:92\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[0;32m     89\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m---> 92\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_step(\n\u001b[0;32m     93\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     94\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     95\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m     96\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     97\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(\n\u001b[0;32m    101\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    102\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m    103\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m    104\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m     )\n",
      "File \u001b[1;32m~\\source\\repos\\waste-classifier\\scripts\\engine.py:23\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     21\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     24\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPUenv\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'module' object"
     ]
    }
   ],
   "source": [
    "from scripts import engine\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "vit_results = engine.train(\n",
    "    model=vit, \n",
    "    train_dataloader=train_dataloader_vit,\n",
    "    test_dataloader=test_dataloader_vit,\n",
    "    epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657cd02-1405-4f5c-a41c-6068b3259ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
