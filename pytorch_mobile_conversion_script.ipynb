{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945a4b98-2b82-4fe2-8aec-a33a7dd64151",
   "metadata": {},
   "source": [
    "# Making a PyTorch model PyTorch Mobile compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b4ccf4-bc9c-4906-b511-53eeab6a9ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n",
      "0.16.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__) # 1.12+\n",
    "print(torchvision.__version__) # 0.13+\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663cdbfb-24cc-4609-9b7f-94fbeccae051",
   "metadata": {},
   "source": [
    "### Custom class for the model\n",
    "* uses Sequential for transforms this time\n",
    "* everything needs to be on cpu now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d1dc010-98fa-4928-a9c2-3fcc277b46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mobile_model(torch.nn.Module):\n",
    "    def __init__(self, model_path=None):\n",
    "        super().__init__()\n",
    "\n",
    "        weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "        self.model = torchvision.models.resnet50(weights=weights)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        num_in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=num_in_features, out_features=6)\n",
    "        )\n",
    "\n",
    "        weights = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model.to('cpu')\n",
    "        self.model.eval()\n",
    "\n",
    "        # Sequential transforms\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            transforms.Resize([256], antialias=True),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        img = self.transforms(tensor)\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to('cpu')\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfe177-19c3-4d2a-9f74-07c24a95a888",
   "metadata": {},
   "source": [
    "### Optimizing for mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2681f71-41e5-4291-9a50-252a9a9b3a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('models/resnet50_model.pth')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models/resnet50_model.pth\")\n",
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a07ea2e8-6dc0-439d-89f1-d120f8eba059",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = mobile_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "880bebbf-6cb1-4474-a8c5-b480dff7597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "scripted_module = torch.jit.script(resnet50_model)\n",
    "optimized_scripted_module = optimize_for_mobile(scripted_module)\n",
    "optimized_scripted_module._save_for_lite_interpreter(\"resnet50_model_lite.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68cfa8fc-aca9-45a3-830e-3fb5949af6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_lite = torch.jit.load(\"resnet50_model_lite.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37414708-55fd-4335-8b22-57c2093bc073",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = Path(\"data/aluminum-cans-1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a738551-e196-405d-8b48-2f06937e294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_img = Image.open(img_path)\n",
    "tensor = transforms.ToTensor()(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66098f7b-dde4-4d31-beca-1b705bfd3140",
   "metadata": {},
   "source": [
    "### Check that results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "457addc8-aca2-48e0-841c-276b5c1a1d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0872, -2.5763, -0.9848, -5.5661, -3.4356, -4.3329]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79617f07-a033-4362-a823-110629a39b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0872, -2.5763, -0.9848, -5.5661, -3.4356, -4.3329]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_lite(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseEnv",
   "language": "python",
   "name": "baseenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
